{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3639a88f-bc95-47b7-93b6-1a52a30c605d",
   "metadata": {},
   "source": [
    "# Task 5 : Build a Simple Chat Application Using Ollama and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38528e0-6582-4c43-afb8-a0527f321ff1",
   "metadata": {},
   "source": [
    "## 01 Using Ollama and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c56a568-6a51-4d17-af1b-8f2adf7c4dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question, or type 'exit'  what is the capital of egypt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abatt\\AppData\\Local\\Temp\\ipykernel_35648\\3256604135.py:9: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = llm(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Egypt is Cairo.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question, or type 'exit'  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit\n"
     ]
    }
   ],
   "source": [
    "# Task 5 part 1\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\",model_kwargs={\"temperature\": 1, \"max_tokens\": 200})\n",
    "while True:\n",
    "    prompt = input(\"Ask your question, or type 'exit' \")\n",
    "    if prompt.lower() == \"exit\":\n",
    "        print(\"Exit\")\n",
    "        break\n",
    "    answer = llm(prompt)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9367bd5-29c0-40ff-8f08-5c189cdd4b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 notable German movies across various historical periods:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"title\": \"Metropolis\",\n",
      "    \"release_year\": 1927,\n",
      "    \"genre\": \"Science Fiction, Drama\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"The White Ribbon\",\n",
      "    \"release_year\": 2009,\n",
      "    \"genre\": \"Drama, History\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Das Boot\",\n",
      "    \"release_year\": 1981,\n",
      "    \"genre\": \"War, Drama\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Good Bye Lenin!\",\n",
      "    \"release_year\": 2003,\n",
      "    \"genre\": \"Comedy, Drama\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"The Lives of Others\",\n",
      "    \"release_year\": 2006,\n",
      "    \"genre\": \"Drama, Thriller\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Lulu and Jimi\",\n",
      "    \"release_year\": 2019,\n",
      "    \"genre\": \"Comedy, Drama\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Germinal\",\n",
      "    \"release_year\":\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# 01 Initialize the Ollama LLM\n",
    "llm = OllamaLLM(model=\"llama3.2\")  #model=\"llama3.2\", model=\"deepseek-coder-v2\", model=\"qwen2\"\n",
    "\n",
    "# Make a request to the model\n",
    "response = llm(\"please find the 10 germany movies in historz, return data as json\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130bb03-a24e-4064-95af-7235c74fd267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b479672a-d5ee-436d-b499-6a3c3ca14bd0",
   "metadata": {},
   "source": [
    "# 02 Using Ollama and Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109cbf98-7e49-48a5-ad37-883af10d9a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question, or type 'exit'  what is the capital of germany\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: It was nice chatting with you, but it seems like you're ready to move on. If you ever need anything or just want to say hello, feel free to come back and start a conversation anytime. Have a great day!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question, or type 'exit'  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit\n"
     ]
    }
   ],
   "source": [
    "# Task 5 part 2\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\",model_kwargs={\"temperature\": 1, \"max_tokens\": 200})\n",
    "messages = [ ChatMessage(role=\"user\", content= prompt)]\n",
    "while True:\n",
    "    prompt = input(\"Ask your question, or type 'exit' \")\n",
    "    if prompt.lower() == \"exit\":\n",
    "        print(\"Exit\")\n",
    "        break\n",
    "    answer = llm.chat(messages)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa644d9-5916-445b-b766-a08428a5a5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question, or type 'exit':  what is the capital of germany\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The capital of Germany is Berlin.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question, or type 'exit':  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(model=\"llama3.2\", model_kwargs={\"temperature\": 1, \"max_tokens\": 200})\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"Ask your question, or type 'exit': \")\n",
    "    if prompt.lower() == \"exit\":\n",
    "        print(\"Exit\")\n",
    "        break\n",
    "    \n",
    "    # Add the user message to the messages list\n",
    "    messages = [(ChatMessage(role=\"user\", content=prompt))]\n",
    "    \n",
    "    # Get the answer from the LLM\n",
    "    answer = llm.chat(messages)\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c4a592-7c4a-48f4-a80e-c924ca7a1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Calculating Factorials in Python**\n",
      "=====================================\n",
      "\n",
      "Below is an example of a Python function that calculates the factorial of a given number.\n",
      "\n",
      "```python\n",
      "def calculate_factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial for.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If n is a negative integer.\n",
      "    \"\"\"\n",
      "\n",
      "    # Check if the input number is negative\n",
      "    if not isinstance(n, int) or n < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer.\")\n",
      "\n",
      "    # Base case for recursion\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "\n",
      "    # Recursive case\n",
      "    else:\n",
      "        return n * calculate_factorial(n - 1)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    num = int(input(\"Enter a number to calculate its factorial: \"))\n",
      "\n",
      "    try:\n",
      "        result = calculate_factorial(num)\n",
      "        print(f\"The factorial of {num} is {result}\")\n",
      "    except ValueError as e:\n",
      "        print(e)\n",
      "```\n",
      "\n",
      "**Explanation**\n",
      "\n",
      "This program defines a function called `calculate_factorial` that takes an integer `n` as input and returns its factorial. The factorial of a number `n`, denoted by `n!`, is the product of all positive integers less than or equal to `n`.\n",
      "\n",
      "The function uses recursion to calculate the factorial:\n",
      "\n",
      "-   If `n` is 0 or 1, it returns 1 (since the factorial of 0 and 1 is defined as 1).\n",
      "-   Otherwise, it calls itself with `n - 1`, multiplies the result by `n`, and returns this product.\n",
      "\n",
      "**Note**: While recursion can be an elegant way to solve problems like this, it may not be the most efficient for large values of `n` due to the overhead of repeated function calls. In such cases, an iterative approach might be more suitable.\n",
      "\n",
      "**Iterative Approach**\n",
      "--------------------\n",
      "\n",
      "Here's an equivalent iterative solution:\n",
      "\n",
      "```python\n",
      "def calculate_factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number iteratively.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial for.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If n is a negative integer.\n",
      "    \"\"\"\n",
      "\n",
      "    # Check if the input number is negative\n",
      "    if not isinstance(n, int) or n < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer.\")\n",
      "\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    num = int(input(\"Enter a number to calculate its factorial: \"))\n",
      "\n",
      "    try:\n",
      "        result = calculate_factorial(num)\n",
      "        print(f\"The factorial of {num} is {result}\")\n",
      "    except ValueError as e:\n",
      "        print(e)\n",
      "```\n",
      "\n",
      "This iterative solution uses a simple loop to multiply all numbers from 2 up to `n` and stores the result in the variable `result`. This approach avoids the overhead of recursive function calls, making it more efficient for large values of `n`."
     ]
    }
   ],
   "source": [
    "### Chat Streaming \n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# Initialize the model\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are helpful assistant to create programs\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Write a python program to calculate the fact of numbers\"),\n",
    "]\n",
    "\n",
    "response = llm.stream_chat(messages)\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f566d766-a5b5-4836-b760-d132283084f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Here are 10 German movies released in 2000 or later, returned in JSON format:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"title\": \"Good Bye Lenin!\",\n",
      "    \"year\": 2003\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"The Lives of Others\",\n",
      "    \"year\": 2006\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Hannah Arendt\",\n",
      "    \"year\": 2012\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Good Bye, Lenin!\",\n",
      "    \"year\": 2003\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"The White Ribbon\",\n",
      "    \"year\": 2009\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Nobody Knows Anything About Anybody\",\n",
      "    \"year\": 2010\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Shadows in the Sun\",\n",
      "    \"year\": 2005\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"The Reader\",\n",
      "    \"year\": 2008\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"In a Year with 13 Moons\",\n",
      "    \"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# Initialize the model\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "messages = [\n",
    " #   ChatMessage(\n",
    "  #      role=\"system\", content=\"You are helpful assistant to create programs\"\n",
    " #   ),\n",
    "    ChatMessage(role=\"user\", content=\"please find the 10 germany movies from 2000, return data as json\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7aed9-5db3-4f3a-adf0-4866d3bd8a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
